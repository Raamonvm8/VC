{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2  \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import mediapipe as mp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clase de configuracion del media pipe para detectar manos\n",
    "class detectorManos():\n",
    "    #define los parámetros necesarios para la detección de MediaPipe\n",
    "    def __init__(self, mode=False, maxManos = 2, model_complexity=0, Confdeteccion = 0.5, Confsegui = 0.5):\n",
    "        self.mode = mode\n",
    "        self.maxManos = maxManos\n",
    "        self.model_complexity = model_complexity\n",
    "        self.Confdeteccion = Confdeteccion\n",
    "        self.Confsegui = Confsegui\n",
    "\n",
    "        self.mpmanos = mp.solutions.hands # carga modulo de mediapipe para detectar manos\n",
    "        self.manos = self.mpmanos.Hands(self.mode, self.maxManos, self.model_complexity, self.Confdeteccion, self.Confsegui) \n",
    "        self.dibujo = mp.solutions.drawing_utils # herramienta para dibujar los puntos clave de la mano\n",
    "        self.tip = [4, 8, 12, 16, 20] # indice de las puntas de los dedos (indice, pulgar, corazon, etc)\n",
    "\n",
    "    #frame de video que encuentra las manos\n",
    "    def encontrarManos(self, frame, dibujar = False):\n",
    "        imgcolor = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        self.resultados = self.manos.process(imgcolor)\n",
    "\n",
    "        if self.resultados.multi_hand_landmarks:\n",
    "            for mano in self.resultados.multi_hand_landmarks:\n",
    "                if dibujar:\n",
    "                    self.dibujo.draw_landmarks(frame, mano, self.mpmanos.HAND_CONNECTIONS)\n",
    "        return frame\n",
    "    \n",
    "    #obtiene las coordenadas de los puntos clave de la mano y devuelve lista con los detalles\n",
    "    def encontrarPosicion(self, frame, ManoNum = 0, dibujar = False):\n",
    "        xlista = []\n",
    "        ylista = []\n",
    "        bbox = []\n",
    "        self.lista = []\n",
    "        if self.resultados.multi_hand_landmarks:\n",
    "            mi_mano = self.resultados.multi_hand_landmarks[ManoNum]\n",
    "            for id, lm in enumerate(mi_mano.landmark):\n",
    "                alto, ancho, c = frame.shape\n",
    "                cx, cy = int(lm.x * ancho), int(lm.y * alto)\n",
    "                xlista.append(cx)\n",
    "                ylista.append(cy)\n",
    "                self.lista.append([id, cx, cy])\n",
    "                if dibujar:\n",
    "                    cv2.circle(frame, (cx, cy), 5, (0, 0, 0), cv2.FILLED)\n",
    "            xmin, xmax = min(xlista), max(xlista)\n",
    "            ymin, ymax = min(ylista), max(ylista)\n",
    "            bbox = xmin, ymin, xmax, ymax\n",
    "            if dibujar:\n",
    "                cv2.rectangle(frame, (xmin - 20, ymin - 20), (xmax + 20, ymax + 20), (0, 255, 0), 2)\n",
    "        return self.lista, bbox\n",
    "    \n",
    "    #detecta los dedos arriba para cada dedo, devuelve lista con 1 si está levantado y 0 si está bajado\n",
    "    def dedosArriba(self):\n",
    "        dedos = []\n",
    "        if self.lista[self.tip[0]][1] < self.lista[self.tip[0]-1][1]:\n",
    "            dedos.append(1)\n",
    "        else:\n",
    "            dedos.append(0)\n",
    "        \n",
    "        for id in range(1,5): \n",
    "            if self.lista[self.tip[id]][2] < self.lista[self.tip[id]-2][2]:\n",
    "                dedos.append(1)\n",
    "            else:\n",
    "                dedos.append(0)\n",
    "        return dedos\n",
    "    \n",
    "    #Devuelve el numero total de dedos levantados\n",
    "    def numeroDedos(self):\n",
    "        if(len(self.lista) != 0):\n",
    "            dedos = self.dedosArriba()\n",
    "            return sum(dedos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1732140327.527124   17591 gl_context.cc:357] GL version: 2.1 (2.1 INTEL-18.7.4), renderer: Intel(R) Iris(TM) Plus Graphics 640\n",
      "I0000 00:00:1732140327.539261   17591 gl_context.cc:357] GL version: 2.1 (2.1 INTEL-18.7.4), renderer: Intel(R) Iris(TM) Plus Graphics 640\n",
      "W0000 00:00:1732140327.545073  280638 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1732140327.565194  280631 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1732140327.585361  280632 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "import os\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "\n",
    "# Ignorar advertencias de protobuf\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module='google.protobuf.symbol_database')\n",
    "\n",
    "# Inicializar detector de manos y MediaPipe Face Detection\n",
    "detector = detectorManos(0.75)\n",
    "mp_face_detection = mp.solutions.face_detection\n",
    "face_detection = mp_face_detection.FaceDetection(min_detection_confidence=0.2)\n",
    "\n",
    "# Iniciar la captura de video\n",
    "vid = cv2.VideoCapture(0)\n",
    "\n",
    "# Configuración para guardar el video de salida\n",
    "output_video_path = os.path.join(os.getcwd(), 'output.mp4')\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "out = cv2.VideoWriter(output_video_path, fourcc, vid.get(cv2.CAP_PROP_FPS),\n",
    "                      (int(vid.get(cv2.CAP_PROP_FRAME_WIDTH)), int(vid.get(cv2.CAP_PROP_FRAME_HEIGHT))))\n",
    "\n",
    "flag = 0\n",
    "\n",
    "while True:\n",
    "    ret, frame = vid.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Detectar manos en el fotograma y obtener la posición en manosInfo\n",
    "    frame = detector.encontrarManos(frame)\n",
    "    manosInfo, cuadro = detector.encontrarPosicion(frame, dibujar=False)\n",
    "\n",
    "    # Detectar número de dedos\n",
    "    if detector.numeroDedos() == 1:\n",
    "        img = cv2.imread('caraRamon.PNG', cv2.IMREAD_UNCHANGED)  # Leer imagen con canal alfa\n",
    "        flag = 1\n",
    "    elif detector.numeroDedos() == 2:\n",
    "        img = cv2.imread('caraNoah.PNG', cv2.IMREAD_UNCHANGED)  # Leer imagen con canal alfa\n",
    "        flag = 1\n",
    "    elif detector.numeroDedos() == 3:\n",
    "        img = cv2.imread('caraCarlos.PNG', cv2.IMREAD_UNCHANGED)  \n",
    "        flag = 1\n",
    "    elif detector.numeroDedos() == 0:\n",
    "        flag = 0\n",
    "\n",
    "    if flag == 1:\n",
    "        # Convertir el fotograma a formato RGB y detectar caras\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        results = face_detection.process(frame_rgb)\n",
    "\n",
    "        if results.detections:\n",
    "            for detection in results.detections:\n",
    "                # Calcular la caja delimitadora de la cara recogiendo coordenadas, ancho y largo, etc\n",
    "                bboxC = detection.location_data.relative_bounding_box\n",
    "                ih, iw, _ = frame.shape \n",
    "                x, y, w, h = int(bboxC.xmin * iw), int(bboxC.ymin * ih), int(bboxC.width * iw), int(bboxC.height * ih)\n",
    "\n",
    "                # Escalar la careta para ajustarla al tamaño de la cara\n",
    "                escala_careta = 1.1\n",
    "                overlay = cv2.resize(img, (int(w * escala_careta), int(h * escala_careta)))\n",
    "\n",
    "                # Se calculan las coordenadas del centro de la cara y se desplaza el overlay para que coincida con el centro de la careta\n",
    "                new_x = max(x + w // 2 - overlay.shape[1] // 2, 0) \n",
    "                new_y = max(y + h // 2 - overlay.shape[0] // 2, 0)\n",
    "\n",
    "                # Compara las dimensiones del overlay con el frame para evitar que se salga del los bordes\n",
    "                h_overlay, w_overlay = overlay.shape[:2]\n",
    "                h_frame, w_frame = frame.shape[:2]\n",
    "\n",
    "                # define el area del frame donde se colocará la careta (overlay)\n",
    "                y1, y2 = new_y, new_y + h_overlay\n",
    "                x1, x2 = new_x, new_x + w_overlay\n",
    "\n",
    "                # Asegurarse de que las dimensiones no excedan los límites del fotograma\n",
    "                y1, y2 = max(0, y1), min(h_frame, y2)\n",
    "                x1, x2 = max(0, x1), min(w_frame, x2)\n",
    "\n",
    "                #Se recorta el overlay para ajustarse la region x1, y1 a x2 y2\n",
    "                overlay = overlay[0:y2 - y1, 0:x2 - x1]\n",
    "\n",
    "                # Superponer la careta usando transparencia\n",
    "                alpha_overlay = overlay[:, :, 3] / 255.0\n",
    "                alpha_frame = 1.0 - alpha_overlay\n",
    "\n",
    "                # bucle que mezcla el overlay con el frame y se suman para hacer el efecto de superposicion\n",
    "                for c in range(3):  # BGR\n",
    "                    frame[y1:y2, x1:x2, c] = (alpha_overlay * overlay[:, :, c] + \n",
    "                                            alpha_frame * frame[y1:y2, x1:x2, c]) \n",
    "\n",
    "\n",
    "    # Mostrar el fotograma con las caretas colocadas\n",
    "    cv2.imshow('Frame', frame)\n",
    "\n",
    "    # Guardar el fotograma en el video de salida\n",
    "    out.write(frame)\n",
    "\n",
    "    # Detener la ejecución con la tecla ESC\n",
    "    if cv2.waitKey(20) == 27:\n",
    "        break\n",
    "\n",
    "# Liberar la captura de video y cerrar las ventanas\n",
    "vid.release()\n",
    "out.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1732134913.272179   17591 gl_context.cc:357] GL version: 2.1 (2.1 INTEL-18.7.4), renderer: Intel(R) Iris(TM) Plus Graphics 640\n",
      "I0000 00:00:1732134913.291055   17591 gl_context.cc:357] GL version: 2.1 (2.1 INTEL-18.7.4), renderer: Intel(R) Iris(TM) Plus Graphics 640\n",
      "I0000 00:00:1732134913.324182   17591 gl_context.cc:357] GL version: 2.1 (2.1 INTEL-18.7.4), renderer: Intel(R) Iris(TM) Plus Graphics 640\n",
      "W0000 00:00:1732134913.354399  205462 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1732134913.354928  205453 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1732134913.367752  205445 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1732134913.434450  205445 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1732134913.435596  205464 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "import os\n",
    "\n",
    "\n",
    "# Ignorar advertencia de protobuf\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module='google.protobuf.symbol_database')\n",
    "\n",
    "# Inicializar detector de manos y MediaPipe Face Mesh\n",
    "detector = detectorManos(0.75)\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "mp_face_detection = mp.solutions.face_detection\n",
    "face_detection = mp_face_detection.FaceDetection(min_detection_confidence=0.2)\n",
    "face_mesh = mp_face_mesh.FaceMesh(min_detection_confidence=0.2)\n",
    "\n",
    "# Iniciar la captura de video\n",
    "vid = cv2.VideoCapture(0)\n",
    "\n",
    "# Configuración para guardar el video de salida\n",
    "output_video_path = os.path.join(os.getcwd(), 'output.mp4') \n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "out = cv2.VideoWriter(output_video_path, fourcc, vid.get(cv2.CAP_PROP_FPS), \n",
    "                      (int(vid.get(cv2.CAP_PROP_FRAME_WIDTH)), int(vid.get(cv2.CAP_PROP_FRAME_HEIGHT))))\n",
    "\n",
    "flag = 0\n",
    "\n",
    "while True:\n",
    "    ret, frame = vid.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    \n",
    "    #Detectar manos en el fotograma y obtener la posicion en manosInfo\n",
    "    frame = detector.encontrarManos(frame)\n",
    "    manosInfo, cuadro = detector.encontrarPosicion(frame, dibujar=False)\n",
    "    \n",
    "    #detectar numero de dedos\n",
    "    if detector.numeroDedos() == 1:\n",
    "        img = cv2.imread('gafas_negras.png')\n",
    "        flag = 1\n",
    "    elif detector.numeroDedos() == 2:\n",
    "        img = cv2.imread('gafas_verdes.png')\n",
    "        flag = 1\n",
    "    elif detector.numeroDedos() == 0:\n",
    "        flag = 0\n",
    "\n",
    "    if flag == 1:\n",
    "        \n",
    "        \n",
    "        #Convierte el fotograma a formato RGB y lo procesa con los módulos de MediaPipe face_mesh y face_detection para detectar la cara \n",
    "        # y los puntos específicos como ojos\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        results = face_mesh.process(frame_rgb)\n",
    "        results2 = face_detection.process(frame_rgb)\n",
    "\n",
    "        if results2.detections: # Si se detecta una cara\n",
    "            for detection in results2.detections:\n",
    "                #Se calcula la caja delimitadora\n",
    "                bboxC = detection.location_data.relative_bounding_box\n",
    "                ih, iw, _ = frame.shape\n",
    "                x, y, w, h = int(bboxC.xmin * iw), int(bboxC.ymin * ih), int(bboxC.width * iw), int(bboxC.height * ih)\n",
    "                \n",
    "        #Si se detectan puntos faciales como ojos\n",
    "        if results.multi_face_landmarks:\n",
    "            for landmarks in results.multi_face_landmarks:\n",
    "                #Se calcula las coordenadas de los ojos y el punto medio entre ellos para colocar las gafas en el centro de la cara\n",
    "                left_eye = landmarks.landmark[33] #indice 33 corresponde a ojo izq\n",
    "                right_eye = landmarks.landmark[133] #indice 133 corresponde a ojo derecho\n",
    "                #coordenadas de los ojos, se multiplican por el ancho y alto para obtener mejor las posiciones en pixeles de cada ojo\n",
    "                left_eye_x = int(left_eye.x * iw)\n",
    "                left_eye_y = int(left_eye.y * ih)\n",
    "                right_eye_x = int(right_eye.x * iw)\n",
    "                right_eye_y = int(right_eye.y * ih)\n",
    "                #Calcula el punto medio de los ojos para usarlo como centro de colocacion de las gafas\n",
    "                center_x = (left_eye_x + right_eye_x) // 2 + 55\n",
    "                center_y = (left_eye_y + right_eye_y) // 2\n",
    "\n",
    "                #Escalado de gafas y resize para tener un tamaño adecuado respecto a la cara\n",
    "                escala_gafas = 1.05\n",
    "                overlay = cv2.resize(img, (int(w * escala_gafas), int(h * escala_gafas)))\n",
    "                new_x = max(center_x - overlay.shape[1] // 2, 0)\n",
    "                new_y = max(center_y - overlay.shape[0] // 2, 0)\n",
    "\n",
    "                #Extrae región del fotograma donde se colocaran las gafas\n",
    "                n_frame = frame[new_y:new_y+overlay.shape[0], new_x:new_x+overlay.shape[1]]\n",
    "                gray_overlay = cv2.cvtColor(overlay, cv2.COLOR_BGR2GRAY)\n",
    "                _, mask = cv2.threshold(gray_overlay, 1, 255, cv2.THRESH_BINARY)\n",
    "                mask_inv = cv2.bitwise_not(mask)\n",
    "\n",
    "                #bg_frame es la región original del fotograma en el área de las gafas, pero haciendo un hueco con forma de gafas\n",
    "                bg_frame = cv2.bitwise_and(n_frame, n_frame, mask=mask_inv)\n",
    "                fg_overlay = cv2.bitwise_and(overlay, overlay, mask=mask) # contiene dsolo las gafas en si\n",
    "                # Se combina el area sin gafas y con gafas\n",
    "                result = cv2.add(bg_frame, fg_overlay)\n",
    "                #coloca result en el frame, sobreescribiendo esa sección para que se vean las gafas en la cara detectada\n",
    "                frame[new_y:new_y+overlay.shape[0], new_x:new_x+overlay.shape[1]] = result\n",
    "\n",
    "    # Mostrar el fotograma con las gafas colocadas\n",
    "    cv2.imshow('Frame', frame)\n",
    "    \n",
    "    # Guardar el fotograma en el video de salida\n",
    "    out.write(frame)\n",
    "    \n",
    "    # Detener la ejecución con la tecla ESC\n",
    "    if cv2.waitKey(20) == 27:\n",
    "        break\n",
    "\n",
    "# Liberar la captura de video y cerrar las ventanas\n",
    "vid.release()\n",
    "out.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "VC_P5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
