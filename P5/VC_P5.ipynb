{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-14 14:34:58.071892: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import cv2  \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import mediapipe as mp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clase de configuracion del media pipe para detectar manos\n",
    "class detectorManos():\n",
    "    def __init__(self, mode=False, maxManos = 2, model_complexity=0, Confdeteccion = 0.5, Confsegui = 0.5):\n",
    "        self.mode = mode\n",
    "        self.maxManos = maxManos\n",
    "        self.model_complexity = model_complexity\n",
    "        self.Confdeteccion = Confdeteccion\n",
    "        self.Confsegui = Confsegui\n",
    "\n",
    "        self.mpmanos = mp.solutions.hands\n",
    "        self.manos = self.mpmanos.Hands(self.mode, self.maxManos, self.model_complexity, self.Confdeteccion, self.Confsegui)\n",
    "        self.dibujo = mp.solutions.drawing_utils\n",
    "        self.tip = [4, 8, 12, 16, 20]\n",
    "\n",
    "    def encontrarManos(self, frame, dibujar = False):\n",
    "        imgcolor = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        self.resultados = self.manos.process(imgcolor)\n",
    "\n",
    "        if self.resultados.multi_hand_landmarks:\n",
    "            for mano in self.resultados.multi_hand_landmarks:\n",
    "                if dibujar:\n",
    "                    self.dibujo.draw_landmarks(frame, mano, self.mpmanos.HAND_CONNECTIONS)\n",
    "        return frame\n",
    "    \n",
    "    def encontrarPosicion(self, frame, ManoNum = 0, dibujar = False):\n",
    "        xlista = []\n",
    "        ylista = []\n",
    "        bbox = []\n",
    "        self.lista = []\n",
    "        if self.resultados.multi_hand_landmarks:\n",
    "            mi_mano = self.resultados.multi_hand_landmarks[ManoNum]\n",
    "            for id, lm in enumerate(mi_mano.landmark):\n",
    "                alto, ancho, c = frame.shape\n",
    "                cx, cy = int(lm.x * ancho), int(lm.y * alto)\n",
    "                xlista.append(cx)\n",
    "                ylista.append(cy)\n",
    "                self.lista.append([id, cx, cy])\n",
    "                if dibujar:\n",
    "                    cv2.circle(frame, (cx, cy), 5, (0, 0, 0), cv2.FILLED)\n",
    "            xmin, xmax = min(xlista), max(xlista)\n",
    "            ymin, ymax = min(ylista), max(ylista)\n",
    "            bbox = xmin, ymin, xmax, ymax\n",
    "            if dibujar:\n",
    "                cv2.rectangle(frame, (xmin - 20, ymin - 20), (xmax + 20, ymax + 20), (0, 255, 0), 2)\n",
    "        return self.lista, bbox\n",
    "    \n",
    "    def dedosArriba(self):\n",
    "        dedos = []\n",
    "        if self.lista[self.tip[0]][1] < self.lista[self.tip[0]-1][1]:\n",
    "            dedos.append(1)\n",
    "        else:\n",
    "            dedos.append(0)\n",
    "        \n",
    "        for id in range(1,5): \n",
    "            if self.lista[self.tip[id]][2] < self.lista[self.tip[id]-2][2]:\n",
    "                dedos.append(1)\n",
    "            else:\n",
    "                dedos.append(0)\n",
    "        return dedos\n",
    "    \n",
    "    def numeroDedos(self):\n",
    "        if(len(self.lista) != 0):\n",
    "            dedos = self.dedosArriba()\n",
    "            return sum(dedos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1731592955.583133 2133096 gl_context.cc:357] GL version: 2.1 (2.1 INTEL-18.7.4), renderer: Intel(R) Iris(TM) Plus Graphics 640\n",
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
      "I0000 00:00:1731592955.723572 2133096 gl_context.cc:357] GL version: 2.1 (2.1 INTEL-18.7.4), renderer: Intel(R) Iris(TM) Plus Graphics 640\n",
      "W0000 00:00:1731592955.733922 2141218 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1731592955.740670 2141241 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1731592955.755302 2141220 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1731592955.759975 2133096 gl_context.cc:357] GL version: 2.1 (2.1 INTEL-18.7.4), renderer: Intel(R) Iris(TM) Plus Graphics 640\n",
      "W0000 00:00:1731592955.766421 2141248 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1731592956.003826 2141245 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1731592963.007223 2141217 landmark_projection_calculator.cc:186] Using NORM_RECT without IMAGE_DIMENSIONS is only supported for the square ROI. Provide IMAGE_DIMENSIONS or use PROJECTION_MATRIX.\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "\n",
    "detector = detectorManos(0.75)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module='google.protobuf.symbol_database') # Ignorar advertencia de protobuf\n",
    "\n",
    "# Inicializar MediaPipe Face Mesh para detectar los puntos de la cara\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "mp_face_detection = mp.solutions.face_detection\n",
    "face_detection = mp_face_detection.FaceDetection(min_detection_confidence=0.2)\n",
    "face_mesh = mp_face_mesh.FaceMesh(min_detection_confidence=0.2)\n",
    "\n",
    "# Iniciar la captura de video\n",
    "vid = cv2.VideoCapture(0)\n",
    "\n",
    "flag=0\n",
    "\n",
    "while(True):\n",
    "    \n",
    "    ret, frame = vid.read()\n",
    "    if not ret:\n",
    "            break\n",
    "    \n",
    "    frame = detector.encontrarManos(frame)\n",
    "    manosInfo, cuadro = detector.encontrarPosicion(frame, dibujar=False)\n",
    "    \n",
    "    if(detector.numeroDedos()== 1):\n",
    "        # Cargar la imagen de las gafas\n",
    "        img = cv2.imread('gafas1.jpg')\n",
    "        flag=1\n",
    "    elif (detector.numeroDedos()==2):\n",
    "        # Cargar la imagen de las gafas\n",
    "        img = cv2.imread('gafas2.jpg')\n",
    "        flag=1\n",
    "    elif (detector.numeroDedos()==0):\n",
    "        flag=0\n",
    "\n",
    "    if(flag==1):\n",
    "\n",
    "        # Convertir las imágenes a escala de grises\n",
    "        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        # Aplicar un umbral para crear imágenes binarias\n",
    "        _, umbral = cv2.threshold(gray, 240, 255, cv2.THRESH_BINARY_INV)\n",
    "        # Encontrar los contornos en la imagen binaria\n",
    "        contornos, _ = cv2.findContours(umbral, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "        # Crear una máscara vacía\n",
    "        mascara = np.zeros_like(img)\n",
    "        # Rellenar las máscaras con el contorno del objeto \n",
    "        cv2.drawContours(mascara, contornos, -1, (255, 255, 255), thickness=cv2.FILLED)\n",
    "        # Extraer lel objeto (gafas)\n",
    "        objeto_extraido = cv2.bitwise_and(img, mascara)\n",
    "        \n",
    "        # Convertir el fotograma a RGB para MediaPipe\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        #Detector de cara\n",
    "        results = face_mesh.process(frame_rgb)\n",
    "        results2 = face_detection.process(frame_rgb)\n",
    "\n",
    "        #solo cordenadas de la fokin cabeza\n",
    "        if results2.detections:\n",
    "            for detection in results2.detections:\n",
    "                # Obtener la caja delimitadora (bounding box) de la cara\n",
    "                bboxC = detection.location_data.relative_bounding_box\n",
    "                ih, iw, _ = frame.shape\n",
    "                x, y, w, h = int(bboxC.xmin * iw), int(bboxC.ymin * ih), int(bboxC.width * iw), int(bboxC.height * ih)\n",
    "                \n",
    "        # detector ojos y facciones dentro de la cara\n",
    "        if results.multi_face_landmarks:\n",
    "            for landmarks in results.multi_face_landmarks:\n",
    "                # Obtener las coordenadas de los ojos izquierdo (punto 33) y derecho (punto 133)\n",
    "                left_eye = landmarks.landmark[33]  # Ojo izquierdo\n",
    "                right_eye = landmarks.landmark[133]  # Ojo derecho\n",
    "\n",
    "                # Convertir las coordenadas normalizadas a píxeles\n",
    "                left_eye_x = int(left_eye.x * iw)\n",
    "                left_eye_y = int(left_eye.y * ih)\n",
    "                right_eye_x = int(right_eye.x * iw)\n",
    "                right_eye_y = int(right_eye.y * ih)\n",
    "                        \n",
    "                # Calcular el punto medio entre los ojos\n",
    "                center_x = (left_eye_x + right_eye_x) // 2 + 35\n",
    "                center_y = (left_eye_y + right_eye_y) // 2\n",
    "                        \n",
    "                # Establecer un factor de escala para las gafas\n",
    "                escala_gafas = 1.2  # Ajustar el tamaño de las gafas\n",
    "                overlay = cv2.resize(objeto_extraido, (int(w * escala_gafas), int(h * escala_gafas)))\n",
    "\n",
    "                # Determinar la nueva posición para colocar las gafas centradas entre los ojos\n",
    "                new_x = center_x - overlay.shape[1] // 2  # Centrado horizontalmente entre los ojos\n",
    "                new_y = center_y - overlay.shape[0] // 2  # Centrado verticalmente entre los ojos\n",
    "\n",
    "                # Asegurarse de que las gafas no se muevan fuera de los límites de la imagen\n",
    "                new_x = max(new_x, 0)\n",
    "                new_y = max(new_y, 0)\n",
    "\n",
    "                # Obtener la región del fotograma donde se colocarán las gafas\n",
    "                n_frame = frame[new_y:new_y+overlay.shape[0], new_x:new_x+overlay.shape[1]]\n",
    "                \n",
    "                # Convertir la imagen de las gafas a escala de grises para obtener la máscara\n",
    "                gray_overlay = cv2.cvtColor(overlay, cv2.COLOR_BGR2GRAY)\n",
    "                        \n",
    "                # Aplicar un umbral a la imagen en escala de grises para generar la máscara\n",
    "                _, mask = cv2.threshold(gray_overlay, 1, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "                # Invertir la máscara\n",
    "                mask_inv = cv2.bitwise_not(mask)\n",
    "                        \n",
    "                # Extraer la parte de la cara (fondo) que no tiene las gafas\n",
    "                bg_frame = cv2.bitwise_and(n_frame, n_frame, mask=mask_inv)\n",
    "                        \n",
    "                # Extraer la parte de las gafas donde se va a colocar\n",
    "                fg_overlay = cv2.bitwise_and(overlay, overlay, mask=mask)\n",
    "                        \n",
    "                # Combinar el fondo de la cara y las gafas en la misma región\n",
    "                result = cv2.add(bg_frame, fg_overlay)\n",
    "                        \n",
    "                # Superponer el resultado en el fotograma original\n",
    "                frame[new_y:new_y+overlay.shape[0], new_x:new_x+overlay.shape[1]] = result\n",
    "\n",
    "    # Mostrar el fotograma con las gafas colocadas\n",
    "    cv2.imshow('Frame', frame)\n",
    "    \n",
    "    # Detener la ejecución con la tecla ESC\n",
    "    if cv2.waitKey(20) == 27:\n",
    "        break\n",
    "\n",
    "# Liberar la captura de video y cerrar las ventanas\n",
    "vid.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1731595026.989983 2189377 gl_context.cc:357] GL version: 2.1 (2.1 INTEL-18.7.4), renderer: Intel(R) Iris(TM) Plus Graphics 640\n",
      "I0000 00:00:1731595027.000638 2189377 gl_context.cc:357] GL version: 2.1 (2.1 INTEL-18.7.4), renderer: Intel(R) Iris(TM) Plus Graphics 640\n",
      "W0000 00:00:1731595027.007227 2212821 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1731595027.015138 2212811 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1731595027.026763 2189377 gl_context.cc:357] GL version: 2.1 (2.1 INTEL-18.7.4), renderer: Intel(R) Iris(TM) Plus Graphics 640\n",
      "W0000 00:00:1731595027.051562 2212829 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1731595027.062248 2212811 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1731595027.099985 2212829 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    },
    {
     "ename": "error",
     "evalue": "OpenCV(4.10.0) /Users/runner/work/opencv-python/opencv-python/opencv/modules/imgproc/src/color.cpp:196: error: (-215:Assertion failed) !_src.empty() in function 'cvtColor'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 40\u001b[0m\n\u001b[1;32m     35\u001b[0m     flag\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m(flag\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m     38\u001b[0m \n\u001b[1;32m     39\u001b[0m     \u001b[38;5;66;03m# Convertir las imágenes a escala de grises\u001b[39;00m\n\u001b[0;32m---> 40\u001b[0m     gray \u001b[38;5;241m=\u001b[39m \u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcvtColor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCOLOR_BGR2GRAY\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;66;03m# Aplicar un umbral para crear imágenes binarias\u001b[39;00m\n\u001b[1;32m     42\u001b[0m     _, umbral \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mthreshold(gray, \u001b[38;5;241m240\u001b[39m, \u001b[38;5;241m255\u001b[39m, cv2\u001b[38;5;241m.\u001b[39mTHRESH_BINARY_INV)\n",
      "\u001b[0;31merror\u001b[0m: OpenCV(4.10.0) /Users/runner/work/opencv-python/opencv-python/opencv/modules/imgproc/src/color.cpp:196: error: (-215:Assertion failed) !_src.empty() in function 'cvtColor'\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "\n",
    "detector = detectorManos(0.75)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module='google.protobuf.symbol_database') # Ignorar advertencia de protobuf\n",
    "\n",
    "# Inicializar MediaPipe Face Mesh para detectar los puntos de la cara\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "mp_face_detection = mp.solutions.face_detection\n",
    "face_detection = mp_face_detection.FaceDetection(min_detection_confidence=0.2)\n",
    "face_mesh = mp_face_mesh.FaceMesh(min_detection_confidence=0.2)\n",
    "\n",
    "# Iniciar la captura de video\n",
    "vid = cv2.VideoCapture(0)\n",
    "\n",
    "flag=0\n",
    "\n",
    "while(True):\n",
    "    \n",
    "    ret, frame = vid.read()\n",
    "    if not ret:\n",
    "            break\n",
    "    \n",
    "    frame = detector.encontrarManos(frame)\n",
    "    manosInfo, cuadro = detector.encontrarPosicion(frame, dibujar=False)\n",
    "    \n",
    "    if(detector.numeroDedos()== 1):\n",
    "        # Cargar la imagen de las gafas\n",
    "        img = cv2.imread('cara_ramon.jpg')\n",
    "        flag=1\n",
    "    elif (detector.numeroDedos()==2):\n",
    "        # Cargar la imagen de las gafas\n",
    "        img = cv2.imread('cara_noah.jpg')\n",
    "        flag=1\n",
    "    elif (detector.numeroDedos()==0):\n",
    "        flag=0\n",
    "\n",
    "    if(flag==1):\n",
    "\n",
    "        # Convertir las imágenes a escala de grises\n",
    "        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        # Aplicar un umbral para crear imágenes binarias\n",
    "        _, umbral = cv2.threshold(gray, 240, 255, cv2.THRESH_BINARY_INV)\n",
    "        # Encontrar los contornos en la imagen binaria\n",
    "        contornos, _ = cv2.findContours(umbral, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "        # Crear una máscara vacía\n",
    "        mascara = np.zeros_like(img)\n",
    "        # Rellenar las máscaras con el contorno del objeto \n",
    "        cv2.drawContours(mascara, contornos, -1, (255, 255, 255), thickness=cv2.FILLED)\n",
    "        # Extraer lel objeto (gafas)\n",
    "        objeto_extraido = cv2.bitwise_and(img, mascara)\n",
    "        \n",
    "        # Convertir el fotograma a RGB para MediaPipe\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        #Detector de cara\n",
    "        results = face_mesh.process(frame_rgb)\n",
    "        results2 = face_detection.process(frame_rgb)\n",
    "\n",
    "        #solo cordenadas de la fokin cabeza\n",
    "        if results2.detections:\n",
    "            for detection in results2.detections:\n",
    "                # Obtener la caja delimitadora (bounding box) de la cara\n",
    "                bboxC = detection.location_data.relative_bounding_box\n",
    "                ih, iw, _ = frame.shape\n",
    "                x, y, w, h = int(bboxC.xmin * iw), int(bboxC.ymin * ih), int(bboxC.width * iw), int(bboxC.height * ih)\n",
    "                \n",
    "        # detector ojos y facciones dentro de la cara\n",
    "        if results2.detections:\n",
    "        for detection in results2.detections:\n",
    "            # Obtener la caja delimitadora (bounding box) de la cara\n",
    "            bboxC = detection.location_data.relative_bounding_box\n",
    "            ih, iw, _ = frame.shape\n",
    "            x, y, w, h = int(bboxC.xmin * iw), int(bboxC.ymin * ih), int(bboxC.width * iw), int(bboxC.height * ih)\n",
    "            \n",
    "            # Calcular el centro de la cara usando el bounding box\n",
    "            center_x = x + w // 2\n",
    "            center_y = y + h // 2\n",
    "            \n",
    "            # Establecer un factor de escala para la imagen de la cara\n",
    "            escala_cara = 1.2  # Ajustar el tamaño de la imagen de la cara\n",
    "            overlay = cv2.resize(objeto_extraido, (int(w * escala_cara), int(h * escala_cara)))\n",
    "\n",
    "            # Determinar la nueva posición para colocar la imagen centrada en la cara\n",
    "            new_x = center_x - overlay.shape[1] // 2\n",
    "            new_y = center_y - overlay.shape[0] // 2\n",
    "\n",
    "            # Asegurarse de que la imagen no se mueva fuera de los límites de la imagen principal\n",
    "            new_x = max(new_x, 0)\n",
    "            new_y = max(new_y, 0)\n",
    "\n",
    "            # Obtener la región del fotograma donde se colocará la imagen de la cara\n",
    "            n_frame = frame[new_y:new_y+overlay.shape[0], new_x:new_x+overlay.shape[1]]\n",
    "            \n",
    "            # Convertir la imagen de la cara a escala de grises para obtener la máscara\n",
    "            gray_overlay = cv2.cvtColor(overlay, cv2.COLOR_BGR2GRAY)\n",
    "                    \n",
    "            # Aplicar un umbral a la imagen en escala de grises para generar la máscara\n",
    "            _, mask = cv2.threshold(gray_overlay, 1, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "            # Invertir la máscara\n",
    "            mask_inv = cv2.bitwise_not(mask)\n",
    "                    \n",
    "            # Extraer la parte de la cara (fondo) que no tiene la imagen sobrepuesta\n",
    "            bg_frame = cv2.bitwise_and(n_frame, n_frame, mask=mask_inv)\n",
    "                    \n",
    "            # Extraer la parte de la imagen que se va a colocar\n",
    "            fg_overlay = cv2.bitwise_and(overlay, overlay, mask=mask)\n",
    "                    \n",
    "            # Combinar el fondo de la cara y la imagen en la misma región\n",
    "            result = cv2.add(bg_frame, fg_overlay)\n",
    "                    \n",
    "            # Superponer el resultado en el fotograma original\n",
    "            frame[new_y:new_y+overlay.shape[0], new_x:new_x+overlay.shape[1]] = result\n",
    "\n",
    "    # Mostrar el fotograma con las gafas colocadas\n",
    "    cv2.imshow('Frame', frame)\n",
    "    \n",
    "    # Detener la ejecución con la tecla ESC\n",
    "    if cv2.waitKey(20) == 27:\n",
    "        break\n",
    "\n",
    "# Liberar la captura de video y cerrar las ventanas\n",
    "vid.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "VC_P5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
